{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torchvision import datasets, transforms\n",
    "#from resnet import *\n",
    "\n",
    "import os\n",
    "import pickle as pk\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from sklearn.metrics import *\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pk.dump(data, f)\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pk.load(f)\n",
    "    return data\n",
    "\n",
    "def gen_todo_list(directory, check = None):\n",
    "    files = os.listdir(directory)\n",
    "    todo_list = []\n",
    "    for f in files:\n",
    "        fullpath = os.path.join(directory, f)\n",
    "        if os.path.isfile(fullpath):\n",
    "            if check is not None:\n",
    "                if check(f):\n",
    "                    todo_list.append(fullpath)\n",
    "            else:\n",
    "                todo_list.append(fullpath)\n",
    "    return todo_list\n",
    "\n",
    "def check(filename):\n",
    "    return not '_class' in filename\n",
    "    \n",
    "\n",
    "def load_data():\n",
    "    max_data_nb = 10000\n",
    "    directory = 'data1d'\n",
    "    todo_list = gen_todo_list(directory, check = check)\n",
    "    ### ver 1 ###\n",
    "    train_rate = 0.64\n",
    "    val_rate = 0.16\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for counter, filename in enumerate(todo_list):\n",
    "        print(filename.split('.')[:-1])\n",
    "        (tmpX, tmpy) = load(filename)\n",
    "        \n",
    "        \n",
    "        tmpy = load('.'.join(filename.split('.')[:-1]) + '_class.pickle')\n",
    "        tmpX , tmpy = tmpX[:max_data_nb], tmpy[:max_data_nb]\n",
    "        assert(len(tmpX) == len(tmpy))\n",
    "        tmpX= processX(tmpX)\n",
    "        \n",
    "        #random.shuffle(tmpX)\n",
    "        \n",
    "        train_num = int(len(tmpX) * train_rate)\n",
    "        val_num = int(len(tmpX) * val_rate)\n",
    "        X_train.extend(tmpX[:train_num])\n",
    "        y_train.extend(tmpy[:train_num])\n",
    "        X_val.extend(tmpX[train_num: train_num + val_num])\n",
    "        y_val.extend(tmpy[train_num: train_num + val_num])\n",
    "        X_test.extend(tmpX[train_num + val_num:])\n",
    "        y_test.extend(tmpy[train_num + val_num:])\n",
    "        print('\\rLoading... {}/{}'.format(counter+1,len(todo_list)), end = '')\n",
    "    print('\\r{} Data loaded.               '.format(len(todo_list)))\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \"\"\"\n",
    "    ### ver2 ###\n",
    "    cpus = mp.cpu_count() - 2\n",
    "    oldtime = time.time()\n",
    "    pool = mp.Pool(processes=cpus)\n",
    "    manager = mp.Manager()\n",
    "    ns = manager.Namespace()\n",
    "    ns.X_train = []\n",
    "    ns.y_train = []\n",
    "    ns.X_val = []\n",
    "    ns.y_val = []\n",
    "    ns.X_test = []\n",
    "    ns.y_test = []\n",
    "\n",
    "    res = pool.map(task, [(ns, i, len(todo_list)) for i in todo_list])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    #pool.apply_async(task, (ns, i,))\n",
    "    newtime = time.time()\n",
    "    print('Using time:', newtime - oldtime, '(sec)')\n",
    "    return ns.X_train, ns.y_train, ns.X_val, ns.y_val, ns.X_test, ns.y_test\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def processX(X):\n",
    "    if True:\n",
    "        \n",
    "        X = np.array(X)\n",
    "        lens = [len(x) for x in X] \n",
    "        maxlen = 1500\n",
    "        tmpX = np.zeros((len(X), maxlen))\n",
    "        mask = np.arange(maxlen) < np.array(lens)[:,None]\n",
    "        tmpX[mask] = np.concatenate(X)\n",
    "        return tmpX\n",
    "        \n",
    "        '''cnt =0\n",
    "        tmpY = []\n",
    "        for i in range(len(X)):\n",
    "            if(len(X[i]) > 2 ):\n",
    "                cnt+=1\n",
    "        tmpX = np.zeros((cnt , 51 , 400))\n",
    "        cnt =0\n",
    "        for i in range(len(X)):\n",
    "            if(len(X[i]) > 2):\n",
    "                for j in range(len(X[i])):\n",
    "                    for k in range(len(X[i][j])):\n",
    "                        tmpX[cnt][j][k] = X[i][j][k]\n",
    "                tmpY.append(Y[i])\n",
    "                cnt+=1\n",
    "        #print(tmpX.shape)\n",
    "        return tmpX , tmpY'''\n",
    "    else:\n",
    "        for i, x in enumerate(X):\n",
    "            tmp_x = np.zeros((1500,))\n",
    "            tmp_x[:len(x)] = x\n",
    "            X[i] = tmp_x\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        \\n        \\n        #self.conv1_drop = nn.Dropout2d(p=0.05)\\n        \\n        self.conv11 = nn.Conv1d(1, 200, kernel_size=5, padding = 1)\\n        self.conv11_bn = nn.BatchNorm1d(200)        \\n        self.conv12 = nn.Conv1d(200, 100, kernel_size=4, padding = 1)\\n        self.conv12_bn = nn.BatchNorm1d(100)        \\n        \\n        \\n        self.fc0 = nn.Linear(74800, 600)\\n        self.fc0_bn = nn.BatchNorm1d(600)\\n        \\n        \\n        self.fc1 = nn.Linear(600, 500)\\n        self.fc1_bn = nn.BatchNorm1d(500)\\n        \\n        self.fc2 = nn.Linear(500, 400)\\n        self.fc2_bn = nn.BatchNorm1d(400)\\n        \\n        self.fc3 = nn.Linear(400, 300)\\n        self.fc3_bn = nn.BatchNorm1d(300)\\n        self.fc4 = nn.Linear(300, 200)\\n        self.fc4_bn = nn.BatchNorm1d(200)\\n        \\n        self.fc5 = nn.Linear(200, 100)\\n        self.fc5_bn = nn.BatchNorm1d(100)\\n        self.fc6 = nn.Linear(100, 50)\\n        self.fc6_bn = nn.BatchNorm1d(50)\\n        self.fc7 = nn.Linear(50, 12)\\n\\n    def forward(self, x):\\n        \\n        x =   self.conv11(x) \\n        x = F.relu(self.conv11_bn(x)  )\\n        x = F.dropout(x, training=self.training,p=0.05)\\n        x = self.conv12(x)\\n        x = F.relu( self.conv12_bn( x ) )\\n        x = F.max_pool1d(  x,2)\\n        x = F.dropout(x, training=self.training,p=0.05)\\n          \\n        \\n        x = x.view( x.size(0),-1)\\n       \\n        x = F.relu(self.fc0_bn( self.fc0(x) ))\\n        \\n        \\n        x = F.relu(self.fc1_bn(self.fc1(x)))\\n        x = F.dropout(x, training=self.training,p=0.05)\\n        \\n        x = F.relu(self.fc2_bn(self.fc2(x)))\\n        x = F.dropout(x, training=self.training,p=0.05)\\n        \\n        x = F.relu(self.fc3_bn(self.fc3(x)))\\n        x = F.dropout(x, training=self.training,p=0.05)\\n        \\n        x = F.relu(self.fc4_bn( self.fc4(x) ))\\n        x = F.dropout(x, training=self.training,p=0.05)\\n        \\n        x = self.fc5(x)\\n        x = F.relu( self.fc5_bn( x) )\\n        x = F.dropout(x, training=self.training,p=0.05)\\n        x = self.fc6(x)\\n        x = F.relu( self.fc6_bn( x) )\\n        x = F.dropout(x, training=self.training,p=0.05)\\n        \\n        \\n        x = self.fc7( x)\\n        return F.log_softmax(x, dim=1)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class Net(nn.Module):\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        node_num = 16\n",
    "        self.conv10 = nn.Conv2d(1, node_num, kernel_size=(3,5) , padding = (1,2))\n",
    "        #self.conv1_drop = nn.Dropout2d(p=0.05)\n",
    "        self.conv10_bn = nn.BatchNorm2d(node_num)\n",
    "        self.conv11 = nn.Conv2d(node_num, node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv11_bn = nn.BatchNorm2d(node_num)        \n",
    "        self.conv12 = nn.Conv2d(node_num, node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv12_bn = nn.BatchNorm2d(node_num)        \n",
    "        self.conv13 = nn.Conv2d(node_num, node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv13_bn = nn.BatchNorm2d(node_num)        \n",
    "        self.conv14 = nn.Conv2d(node_num, node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv14_bn = nn.BatchNorm2d(node_num)        \n",
    "        self.conv15 = nn.Conv2d(node_num, node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv15_bn = nn.BatchNorm2d(node_num)        \n",
    "        self.conv16 = nn.Conv2d(node_num, node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv16_bn = nn.BatchNorm2d(node_num)\n",
    "        \n",
    "        self.conv20 = nn.Conv2d(node_num ,2*node_num, kernel_size=1, padding = 0)\n",
    "        self.conv20_bn = nn.BatchNorm2d(2*node_num)   \n",
    "        self.conv21 = nn.Conv2d(node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv21_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv22 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv22_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv23 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv23_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv24 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv24_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv25 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv25_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv26 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv26_bn = nn.BatchNorm2d(2*node_num)\n",
    "        self.conv27 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv27_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv28 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv28_bn = nn.BatchNorm2d(2*node_num)\n",
    "        self.conv29 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv29_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv2a = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv2a_bn = nn.BatchNorm2d(2*node_num)\n",
    "        \n",
    "        #self.conv30 = nn.Conv2d(2*node_num ,2*node_num, kernel_size=1, padding = 0)\n",
    "        #self.conv30_bn = nn.BatchNorm2d(2*node_num)   \n",
    "        self.conv31 = nn.Conv2d(2*node_num ,2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv31_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv32 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv32_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv33 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv33_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv34 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv34_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv35 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv35_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv36 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv36_bn = nn.BatchNorm2d(2*node_num)\n",
    "        self.conv37 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv37_bn = nn.BatchNorm2d(2*node_num)        \n",
    "        self.conv38 = nn.Conv2d(2*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv38_bn = nn.BatchNorm2d(2*node_num)\n",
    "        \n",
    "        self.conv40 = nn.Conv2d(2*node_num ,4*node_num, kernel_size=1, padding = 0,stride = (2,2))\n",
    "        self.conv40_bn = nn.BatchNorm2d(4*node_num)   \n",
    "        self.conv41 = nn.Conv2d(2*node_num ,4*node_num, kernel_size=(3,3), padding = 1,stride = (2,2))\n",
    "        self.conv41_bn = nn.BatchNorm2d(4*node_num)        \n",
    "        self.conv42 = nn.Conv2d(4*node_num, 4*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv42_bn = nn.BatchNorm2d(4*node_num)        \n",
    "        self.conv43 = nn.Conv2d(4*node_num, 4*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv43_bn = nn.BatchNorm2d(4*node_num)        \n",
    "        self.conv44 = nn.Conv2d(4*node_num, 4*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv44_bn = nn.BatchNorm2d(4*node_num)        \n",
    "        self.conv45 = nn.Conv2d(4*node_num, 4*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv45_bn = nn.BatchNorm2d(4*node_num)        \n",
    "        self.conv46 = nn.Conv2d(4*node_num, 2*node_num, kernel_size=(3,3), padding = 1)\n",
    "        self.conv46_bn = nn.BatchNorm2d(2*node_num)\n",
    "        \n",
    "        \n",
    "        #self.flat_bn = nn.BatchNorm1d(11880)\n",
    "        \n",
    "        self.fc0 = nn.Linear(19200, 600)\n",
    "        self.fc0_bn = nn.BatchNorm1d(600)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(600, 500)\n",
    "        self.fc1_bn = nn.BatchNorm1d(500)\n",
    "        \n",
    "        self.fc2 = nn.Linear(500, 400)\n",
    "        self.fc2_bn = nn.BatchNorm1d(400)\n",
    "        \n",
    "        self.fc3 = nn.Linear(400, 300)\n",
    "        self.fc3_bn = nn.BatchNorm1d(300)\n",
    "        self.fc4 = nn.Linear(300, 200)\n",
    "        self.fc4_bn = nn.BatchNorm1d(200)\n",
    "        \n",
    "        self.fc5 = nn.Linear(200, 100)\n",
    "        self.fc5_bn = nn.BatchNorm1d(100)\n",
    "        self.fc6 = nn.Linear(100, 50)\n",
    "        self.fc6_bn = nn.BatchNorm1d(50)\n",
    "        self.fc7 = nn.Linear(50, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x =   self.conv10(x) \n",
    "        x = F.leaky_relu(self.conv10_bn(x)  )\n",
    "        res = x\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv11(x)\n",
    "        x = F.leaky_relu( self.conv11_bn( x ) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)\n",
    "        x = self.conv12(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.leaky_relu( self.conv12_bn( x )  )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv13(x)\n",
    "        x = F.leaky_relu( self.conv13_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv14(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.leaky_relu( self.conv14_bn( x  ) )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv15(x)\n",
    "        x = F.leaky_relu( self.conv15_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv16(x)\n",
    "        x = x + res\n",
    "        res = self.conv20_bn(self.conv20(x))\n",
    "        x = F.leaky_relu( self.conv16_bn( x ))\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        ####################################################\n",
    "        x =  self.conv21(x)\n",
    "        x = F.relu(self.conv21_bn( x ) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)\n",
    "        x = self.conv22(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.relu( self.conv22_bn( x ) )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv23(x)\n",
    "        x = F.relu( self.conv23_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv24(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.relu( self.conv24_bn( x )  )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv25(x)\n",
    "        x = F.relu( self.conv25_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv26(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.relu( self.conv26_bn( x )  )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv27(x)\n",
    "        x = F.relu( self.conv27_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv28(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.relu( self.conv28_bn( x ) + res )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv29(x)\n",
    "        x = F.relu( self.conv29_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv2a(x)\n",
    "        x = x + res\n",
    "        res =x\n",
    "        x = F.relu( self.conv2a_bn( x )  )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        ###################################################################\n",
    "        x =  self.conv31(x)\n",
    "        x = F.leaky_relu(self.conv31_bn( x ) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)\n",
    "        x = self.conv32(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.leaky_relu( self.conv32_bn( x ) )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv33(x)\n",
    "        x = F.leaky_relu( self.conv33_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv34(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.leaky_relu( self.conv34_bn( x ))\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv35(x)\n",
    "        x = F.leaky_relu( self.conv35_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv36(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.leaky_relu( self.conv36_bn( x ) +res )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv37(x)\n",
    "        x = F.leaky_relu( self.conv37_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv38(x)\n",
    "        x = x + res\n",
    "        res =self.conv40_bn( self.conv40(x))\n",
    "        x = F.leaky_relu( self.conv38_bn( x )  )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        ########################################################\n",
    "        x =  self.conv41(x)\n",
    "        x = F.relu(self.conv41_bn( x ) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)\n",
    "        x = self.conv42(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.relu( self.conv42_bn( x ) )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv43(x)\n",
    "        x = F.relu( self.conv43_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv44(x)\n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = F.relu( self.conv44_bn( x )  )\n",
    "        #x = F.dropout(x, training=self.training,p=0.1)\n",
    "        x = self.conv45(x)\n",
    "        x = F.relu( self.conv45_bn( x) )\n",
    "        x = F.dropout2d(x, training=self.training,p=0.1)  \n",
    "        x = self.conv46(x)\n",
    "        x = F.max_pool2d(  F.relu( self.conv46_bn( x )  ),2)\n",
    "          \n",
    "        \n",
    "        x = x.view( x.size(0),-1)\n",
    "       \n",
    "        x = F.relu(self.fc0_bn( self.fc0(x) ))\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = F.dropout(x, training=self.training,p=0.15)\n",
    "        \n",
    "        x = F.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = F.dropout(x, training=self.training,p=0.15)\n",
    "        \n",
    "        x = F.relu(self.fc3_bn(self.fc3(x)))\n",
    "        x = F.dropout(x, training=self.training,p=0.15)\n",
    "        \n",
    "        x = F.relu(self.fc4_bn( self.fc4(x) ))\n",
    "        x = F.dropout(x, training=self.training,p=0.15)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = F.relu( self.fc5_bn( x) )\n",
    "        x = F.dropout(x, training=self.training,p=0.15)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu( self.fc6_bn( x) )\n",
    "        x = F.dropout(x, training=self.training,p=0.15)\n",
    "        \n",
    "        \n",
    "        x = self.fc7( x)\n",
    "        return F.log_softmax(x, dim=1)'''\n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        #self.conv1_drop = nn.Dropout2d(p=0.05)\n",
    "        \n",
    "        self.conv11 = nn.Conv1d(1, 200, kernel_size=5, padding = 1)\n",
    "        self.conv11_bn = nn.BatchNorm1d(200)        \n",
    "        self.conv12 = nn.Conv1d(200, 100, kernel_size=4, padding = 1)\n",
    "        self.conv12_bn = nn.BatchNorm1d(100)        \n",
    "        \n",
    "        \n",
    "        self.fc0 = nn.Linear(74800, 600)\n",
    "        self.fc0_bn = nn.BatchNorm1d(600)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(600, 500)\n",
    "        self.fc1_bn = nn.BatchNorm1d(500)\n",
    "        \n",
    "        self.fc2 = nn.Linear(500, 400)\n",
    "        self.fc2_bn = nn.BatchNorm1d(400)\n",
    "        \n",
    "        self.fc3 = nn.Linear(400, 300)\n",
    "        self.fc3_bn = nn.BatchNorm1d(300)\n",
    "        self.fc4 = nn.Linear(300, 200)\n",
    "        self.fc4_bn = nn.BatchNorm1d(200)\n",
    "        \n",
    "        self.fc5 = nn.Linear(200, 100)\n",
    "        self.fc5_bn = nn.BatchNorm1d(100)\n",
    "        self.fc6 = nn.Linear(100, 50)\n",
    "        self.fc6_bn = nn.BatchNorm1d(50)\n",
    "        self.fc7 = nn.Linear(50, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x =   self.conv11(x) \n",
    "        x = F.relu(self.conv11_bn(x)  )\n",
    "        x = F.dropout(x, training=self.training,p=0.05)\n",
    "        x = self.conv12(x)\n",
    "        x = F.relu( self.conv12_bn( x ) )\n",
    "        x = F.max_pool1d(  x,2)\n",
    "        x = F.dropout(x, training=self.training,p=0.05)\n",
    "          \n",
    "        \n",
    "        x = x.view( x.size(0),-1)\n",
    "       \n",
    "        x = F.relu(self.fc0_bn( self.fc0(x) ))\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = F.dropout(x, training=self.training,p=0.05)\n",
    "        \n",
    "        x = F.relu(self.fc2_bn(self.fc2(x)))\n",
    "        x = F.dropout(x, training=self.training,p=0.05)\n",
    "        \n",
    "        x = F.relu(self.fc3_bn(self.fc3(x)))\n",
    "        x = F.dropout(x, training=self.training,p=0.05)\n",
    "        \n",
    "        x = F.relu(self.fc4_bn( self.fc4(x) ))\n",
    "        x = F.dropout(x, training=self.training,p=0.05)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = F.relu( self.fc5_bn( x) )\n",
    "        x = F.dropout(x, training=self.training,p=0.05)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu( self.fc6_bn( x) )\n",
    "        x = F.dropout(x, training=self.training,p=0.05)\n",
    "        \n",
    "        \n",
    "        x = self.fc7( x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('models/res_50_aaa.pickle', 'rb') as f:\n",
    "    model = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    ypred =np.array( [])\n",
    "    ytrue = np.array( [])\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            ypred = np.append(ypred,output.max(1, keepdim=True)[1].cpu().numpy().astype(np.int))\n",
    "            ytrue = np.append(ytrue,target.cpu().numpy().astype(np.int))\n",
    "    print(ypred)\n",
    "    print(ytrue)\n",
    "    cm = confusion_matrix(ytrue, ypred)        \n",
    "    \n",
    "    return cm\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pk.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.  3.  8. ...  8. 10.  5.]\n",
      "[10.  3.  8. ...  8. 10.  5.]\n",
      "[[ 188    2    3    3    0    0    0    0    0    0    0    1]\n",
      " [   5   69    0    1    0    0    0    0    0    0    0    0]\n",
      " [   1    0  930    6    0    0    0    0    0    0    0    2]\n",
      " [   1    0    2 1478    0    0    0    0    0    0    0    2]\n",
      " [   0    0    0    0  193    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0  140    0    3    0    0    2    0]\n",
      " [   0    0    0    0    0    0   52    0    0    0    0    0]\n",
      " [   0    0    0    0    0    3    0  295    0    0    2    0]\n",
      " [   0    0    0    0    0    0    0    0  268    0    2    0]\n",
      " [   0    0    0    0    0    1    1    0    0  358    0    0]\n",
      " [   0    0    0    0    0    3    0    0    8    0  343    0]\n",
      " [   2    0    4    4    0    0    0    0    0    0    0 1225]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''#X_val = load('X_val.pickle')\n",
    "#y_val_onehot = load('y_val.pickle')\n",
    "# Training settings\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data()\n",
    "# normalize X\n",
    "X_train , X_val, X_test = np.array(X_train) / 255, np.array(X_val) / 255, np.array(X_test) / 255\n",
    "# 把 y 的 string 做成 one hot encoding 形式\n",
    "label_encoder = LabelBinarizer()\n",
    "y_train_onehot = label_encoder.fit_transform(y_train)\n",
    "y_train_onehot = np.array([np.where(r==1)[0][0] for r in y_train_onehot])\n",
    "y_val_onehot = label_encoder.transform(y_val)\n",
    "y_val_onehot = np.array([np.where(r==1)[0][0] for r in  y_val_onehot])\n",
    "y_test_onehot = label_encoder.transform(y_test)\n",
    "y_test_onehot = np.array([np.where(r==1)[0][0] for r in  y_test_onehot])\n",
    "# 印一些有的沒的\n",
    "print('X_train size:', len(X_train))\n",
    "max_x = 0\n",
    "for x in X_train:\n",
    "    if max_x < len(x):\n",
    "        max_x = len(x)\n",
    "print('max length:',max_x)\n",
    "X_train, X_val,X_test = np.expand_dims(X_train, 1), np.expand_dims(X_val, 1),np.expand_dims(X_test, 1)    \n",
    "#traindata = Data.TensorDataset(torch.from_numpy(X_train).float(),  torch.from_numpy(y_train_onehot.astype(np.int64)))'''\n",
    "X_val = load('training_data/X_val_ff.pickle')\n",
    "y_val_onehot = load('training_data/y_val_ff.pickle')\n",
    "'''for i in range(len(y_val_onehot)):\n",
    "    if(y_val_onehot[i] <= 4 or y_val_onehot[i] == 11):\n",
    "        y_val_onehot[i] = 0\n",
    "    else:\n",
    "        y_val_onehot[i] = 1'''\n",
    "testdata = Data.TensorDataset(torch.from_numpy(X_val).float(),  torch.from_numpy(y_val_onehot.astype(np.int64)))\n",
    "    \n",
    "use_cuda = True\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': False} if use_cuda else {}\n",
    "#train_loader = torch.utils.data.DataLoader(\n",
    "#    traindata,\n",
    "#    batch_size=50, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testdata,\n",
    "    batch_size=100, shuffle=True, **kwargs)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer = optim.Adam(model.parameters())\n",
    "cm = test(model, device, test_loader)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = cm.astype('int')\n",
    "np.savetxt('result/res_50_aaa.csv',cm,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
